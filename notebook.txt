Sunday 27th May 2018

Initial shot. I'm planning to take glove vectors and implement k-means clustering and dimension reduction.

After that I will try and learn hamming distance bit-vectors for specific clusters

Results for baseline. 100k words, 95% training the rest asking for the closest word in the db

r@: 10, 100, 1000: 0.470705858828 0.76524695061 0.956808638272

BTW for 10k words, 50% training the results were:

r@: 10, 100, 1000: 0.683663267347 0.943411317736 0.998800239952

For 1k, 95% train r@10 is 0.764

Checking in with:

First basline on glove

Tuesday 29th May 2018

Checking in with:

glove.py initializes mat_sel to very poor result.

Turns out that there is a problem with the idea. We can't allow the bit vectors to converge to zero distance
or the whole model collapses to the same bits.

I'll try to have the bitvector distance converge to the CS distance. However, first, I want to make sure that the
sel mat learning we are using works. So the fiorst stage is to try and learn the baseline bitdb

Scrub that. I'll try another time at setting the target bits. This time, one bit at a time, aiming to keep an HD
similar to the CD

Thursday 31st May

Well at least one thing works. A nice run with kmeans.

Checking in with:

kmeans without dimred working in glove_kmeans.py